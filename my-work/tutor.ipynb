{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74b9109-ef21-4068-9a5b-15e0162742e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af89faa8-0361-45d7-82ad-e27ad550ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acaba5f5-44ea-409f-95da-e297efad9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f20cb69-26f7-4b60-9083-c0c31298a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37fde69d-371e-4dbb-b3ff-23dd2596d2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay! Imagine you have a box of toys (we can think of this as a list of books), and each toy (book) has a tag on it that says who made it (the author). \n",
       "\n",
       "Now, let’s break down the code piece by piece:\n",
       "\n",
       "1. **`for book in books`**: This part is like saying, \"Let's look at each toy (book) in our big box (list called `books`).\"\n",
       "\n",
       "2. **`book.get(\"author\")`**: Here, we're trying to read the tag on each toy (getting the author's name from each book).\n",
       "\n",
       "3. **`if book.get(\"author\")`**: This is like saying, \"Only if the toy (book) has a tag (author's name), let’s pay attention to it.\" So, we ignore any toys (books) that don’t have a tag.\n",
       "\n",
       "4. **`{...}`**: This part creates a special collection called a \"set.\" A set is like a special bag where you can only keep one of each toy (no duplicates). So, if two books have the same author, we only keep that author in the bag one time.\n",
       "\n",
       "5. **`yield from`**: This is like saying, \"Now, let's take each toy (item) out of our special bag (set) one by one and show it to everyone.\"\n",
       "\n",
       "So, putting it all together, this code looks through the list of books, finds the authors, ignores any that don't have an author, and then gives back a list of the unique authors, one at a time!\n",
       "\n",
       "In short, it finds all the authors from a list of books, makes sure there’s no repeats, and hands them out one by one."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a tutor and will explain things as simply as you can.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = result.choices[0].message.content\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc195c4-e3c2-4dec-85bd-fa371607d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab2c40-c959-437f-adbe-e3289cf7cb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
